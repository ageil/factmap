{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AsterixDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T22:53:50.712890Z",
     "start_time": "2019-05-28T22:53:50.603599Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from asterixdb.asterixdb import AsterixConnection\n",
    "from glob import glob\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Asterix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establish connection to AsterixDB:\n",
    "\n",
    "(using https://github.com/j-goldsmith/asterixdb-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T22:53:51.814678Z",
     "start_time": "2019-05-28T22:53:51.811297Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "con = AsterixConnection(server='http://localhost', port=19002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-24T21:17:59.547107Z",
     "start_time": "2019-04-24T21:17:59.539538Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# response = con.query('''\n",
    "#     USE TinySocial;\n",
    "\n",
    "#     SELECT VALUE user\n",
    "#     FROM GleambookUsers user\n",
    "#     WHERE user.id = 8;''')\n",
    "\n",
    "# response.results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clear dataverse if it already exists(!) and create new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-08T20:58:45.099601Z",
     "start_time": "2019-05-08T20:58:44.749661Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# response = con.query('''\n",
    "#     DROP DATAVERSE FactMap IF EXISTS;\n",
    "#     CREATE DATAVERSE FactMap;\n",
    "#     ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create subset of only 2019 submissions for experimentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T21:23:29.607771Z",
     "start_time": "2019-05-28T21:23:29.600309Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rt_paths = sorted(glob(\"/Users/anders1991/Github/FactMap/Data/reddit/2019/*.json\"))\n",
    "combined = \"\"\n",
    "\n",
    "for p in rt_paths:\n",
    "    combined += 'localhost://' + p + ','\n",
    "    \n",
    "combined = combined[:-1]  # remove last ','"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T21:25:05.741736Z",
     "start_time": "2019-05-28T21:23:30.390540Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q = '''\n",
    "    USE FactMap;\n",
    "    \n",
    "    DROP TYPE submissionTypeTemp IF EXISTS;\n",
    "    CREATE TYPE submissionTypeTemp as {{\n",
    "        uid: uuid\n",
    "    }};\n",
    "    \n",
    "    DROP DATASET PostsTemp IF EXISTS;\n",
    "    CREATE DATASET PostsTemp(submissionTypeTemp)\n",
    "        PRIMARY KEY uid AUTOGENERATED;\n",
    "                \n",
    "    LOAD DATASET PostsTemp\n",
    "    USING localfs ((\"path\"=\"{0}\"),(\"format\"=\"json\"));\n",
    "    '''.format(combined)\n",
    "\n",
    "response = con.query(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cast to intended data format and clean up temp table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T21:26:30.696181Z",
     "start_time": "2019-05-28T21:25:05.747100Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response = con.query('''\n",
    "    USE FactMap;\n",
    "\n",
    "    DROP DATASET posts2019 IF EXISTS;\n",
    "    \n",
    "    CREATE TYPE SubmissionType as {\n",
    "        id: string\n",
    "    };\n",
    "    CREATE DATASET posts2019(SubmissionType)\n",
    "        PRIMARY KEY id;\n",
    "\n",
    "    INSERT INTO posts2019\n",
    "    SELECT \n",
    "    id,\n",
    "    datetime_from_unix_time_in_secs(int(created_utc)) as created_utc,\n",
    "    subreddit_id,\n",
    "    subreddit,\n",
    "    author,\n",
    "    domain,\n",
    "    int(score) as score,\n",
    "    int(num_comments) as num_comments,\n",
    "    title,\n",
    "    url\n",
    "    FROM PostsTemp p;\n",
    "    \n",
    "    DROP DATASET PostsTemp IF EXISTS;\n",
    "    DROP TYPE SubmissionTypeTemp IF EXISTS;\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load all reddit submissions with direct load (note BigQuery gives `int` as `string` so have to cast):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T21:26:58.987910Z",
     "start_time": "2019-05-28T21:26:58.955754Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rt_paths = sorted(glob(\"/Users/anders1991/Github/FactMap/Data/reddit/201*/*.json\"))\n",
    "combined = \"\"\n",
    "\n",
    "for p in rt_paths:\n",
    "    combined += 'localhost://' + p + ','\n",
    "    \n",
    "combined = combined[:-1]  # remove last ,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T21:54:39.568404Z",
     "start_time": "2019-05-28T21:27:00.249078Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q = '''\n",
    "    USE FactMap;\n",
    "    \n",
    "    DROP TYPE submissionTypeTemp IF EXISTS;\n",
    "    CREATE TYPE submissionTypeTemp as {{\n",
    "        uid: uuid\n",
    "    }};\n",
    "    \n",
    "    DROP DATASET PostsTemp IF EXISTS;\n",
    "    CREATE DATASET PostsTemp(submissionTypeTemp)\n",
    "        PRIMARY KEY uid AUTOGENERATED;\n",
    "                \n",
    "    LOAD DATASET PostsTemp\n",
    "    USING localfs ((\"path\"=\"{0}\"),(\"format\"=\"json\"));\n",
    "    '''.format(combined)\n",
    "\n",
    "response = con.query(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cast to intended data format and clean up temp table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T22:28:37.784913Z",
     "start_time": "2019-05-28T21:54:39.582554Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response = con.query('''\n",
    "    USE FactMap;\n",
    "\n",
    "    DROP DATASET posts IF EXISTS;\n",
    "\n",
    "    CREATE DATASET posts(SubmissionType)\n",
    "        PRIMARY KEY id;\n",
    "\n",
    "    INSERT INTO posts\n",
    "    SELECT \n",
    "        id,\n",
    "        datetime_from_unix_time_in_secs(int(created_utc)) as created_utc,\n",
    "        subreddit_id,\n",
    "        subreddit,\n",
    "        author,\n",
    "        domain,\n",
    "        int(score) as score,\n",
    "        int(num_comments) as num_comments,\n",
    "        title,\n",
    "        url\n",
    "    FROM PostsTemp p;\n",
    "    \n",
    "    DROP DATASET PostsTemp IF EXISTS;\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ClaimReview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all claims, generate uid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T22:29:44.741465Z",
     "start_time": "2019-05-28T22:29:40.623371Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = con.query('''\n",
    "    USE FactMap;\n",
    "    \n",
    "    DROP TYPE ReviewsType IF EXISTS;\n",
    "    CREATE TYPE ReviewsType as {\n",
    "        uid: uuid\n",
    "    };\n",
    "    \n",
    "    DROP DATASET claims IF EXISTS;\n",
    "    CREATE DATASET claims(ReviewsType)\n",
    "        PRIMARY KEY uid AUTOGENERATED;\n",
    "                \n",
    "    LOAD DATASET claims\n",
    "    USING localfs ((\"path\"=\"localhost:///Users/anders1991/Github/FactMap/Data/claimreviews/claims_May-01-2019.json\"),(\"format\"=\"json\"));\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up formatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T22:29:47.707151Z",
     "start_time": "2019-05-28T22:29:46.253717Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response = con.query('''\n",
    "    USE FactMap;\n",
    "    \n",
    "    CREATE DATASET reviews(ReviewsType)\n",
    "        PRIMARY KEY uid;\n",
    "\n",
    "    INSERT INTO reviews\n",
    "    SELECT \n",
    "        uid,\n",
    "        reviewUrl,\n",
    "        claimReviewed,\n",
    "        countries,\n",
    "        claimReviewed_en,\n",
    "        datetime_from_unix_time_in_secs(claimDate) as claimDate,\n",
    "        datetime_from_unix_time_in_secs(reviewDate) as reviewDate,\n",
    "        reviewAuthor,\n",
    "        reviewRating,\n",
    "        claimAuthor,\n",
    "        tagsRaw,\n",
    "        tagsNamed,\n",
    "        reviewTitle\n",
    "    FROM claims c;\n",
    "    \n",
    "    DROP DATASET claims IF EXISTS;\n",
    "    ''')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save subset with valid/invalid ratings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T22:29:57.805538Z",
     "start_time": "2019-05-28T22:29:57.787700Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T22:30:02.270190Z",
     "start_time": "2019-05-28T22:29:58.200404Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25477"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rated = con.query('''\n",
    "    USE FactMap;\n",
    "    \n",
    "    SELECT r.*\n",
    "    FROM reviews r\n",
    "    WHERE r.reviewRating.bestRating >= r.reviewRating.ratingValue\n",
    "    AND r.reviewRating.ratingValue >= r.reviewRating.worstRating\n",
    "    AND r.reviewRating.bestRating > r.reviewRating.worstRating;\n",
    "''').results\n",
    "\n",
    "with open('/Users/anders1991/Github/FactMap/RNN/data/rated.pickle', 'wb') as f:\n",
    "    pickle.dump(rated, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "len(rated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T22:30:08.660512Z",
     "start_time": "2019-05-28T22:30:06.199344Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34597"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unrated = con.query('''\n",
    "    USE FactMap;\n",
    "    \n",
    "    SELECT r.*\n",
    "    FROM reviews r\n",
    "    WHERE NOT (r.reviewRating.bestRating >= r.reviewRating.ratingValue\n",
    "    AND r.reviewRating.ratingValue >= r.reviewRating.worstRating\n",
    "    AND r.reviewRating.bestRating > r.reviewRating.worstRating)\n",
    "    OR is_null(r.reviewRating.ratingValue)\n",
    "    OR is_null(r.reviewRating.worstRating)\n",
    "    OR is_null(r.reviewRating.bestRating);\n",
    "''').results\n",
    "\n",
    "with open('/Users/anders1991/Github/FactMap/RNN/data/unrated.pickle', 'wb') as f:\n",
    "    pickle.dump(unrated, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "len(unrated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vectors (FastText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: See vec2json2asterix.py for more efficient pipeline and split original file into 4 using:\n",
    "\n",
    "```\n",
    "split -l 500000 crawl_300d_2M.vec crawl_300d_2M\n",
    "```\n",
    "\n",
    "Then run vec2json.py on the resulting files (in parallel), and use json2asterix.py to push them to the database (in parallel)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "USE FactMap;\n",
    "\n",
    "DROP DATASET fasttext IF EXISTS;\n",
    "DROP TYPE EmbeddingType IF EXISTS;\n",
    "\n",
    "CREATE TYPE EmbeddingType AS {\n",
    "    word: string,\n",
    "    vector: [float]\n",
    "};\n",
    "\n",
    "CREATE DATASET fasttext(EmbeddingType)\n",
    "    PRIMARY KEY word;\n",
    "\n",
    "INSERT INTO fasttext\n",
    "    ([\n",
    "        {\"word\": \"hello\", \"vector\": [0.0, 0.1]}\n",
    "    ]);\n",
    "\n",
    "SELECT *\n",
    "FROM fasttext;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Facebook Research's 300-dimensional vectors for 2M words trained on Common Crawl:\n",
    "https://fasttext.cc/docs/en/english-vectors.html\n",
    "\n",
    "Importantly, these were not trained on Wikipedia but on socially 'flawed' words (like our own data)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T05:40:24.897606Z",
     "start_time": "2019-05-05T05:40:24.887786Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = map(float, tokens[1:])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T05:57:11.368170Z",
     "start_time": "2019-05-05T05:40:28.778975Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname = \"/Users/anders1991/Github/FactMap/data/fasttext/crawl-300d-2M.vec\"\n",
    "\n",
    "vectors = load_vectors(fname) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T06:44:01.616301Z",
     "start_time": "2019-05-09T06:44:00.730988Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response = con.query('''\n",
    "    USE FactMap;\n",
    "\n",
    "    DROP DATASET fasttext IF EXISTS;\n",
    "    DROP TYPE EmbeddingType IF EXISTS;\n",
    "\n",
    "    CREATE TYPE EmbeddingType AS {\n",
    "        word: string,\n",
    "        vector: [float]\n",
    "    };\n",
    "\n",
    "    CREATE DATASET fasttext(EmbeddingType)\n",
    "        PRIMARY KEY word;\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert word embeddings into dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-06T17:24:59.631279Z",
     "start_time": "2019-05-06T17:24:59.603330Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "failed = []\n",
    "\n",
    "for w, v in vectors.items():\n",
    "    try:\n",
    "        word = w\n",
    "        vec = *v,\n",
    "        response = con.query('''\n",
    "            USE FactMap;\n",
    "            \n",
    "            INSERT INTO fasttext\n",
    "            ([{{\"word\":\"{0}\", \"vector\":{1}}}])\n",
    "        '''.format(word, list(vec)))\n",
    "    except:\n",
    "        failed.append(w)\n",
    "\n",
    "# print(\"Failed word inserts:\", len(failed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard join (claims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join on full source URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T22:32:58.846281Z",
     "start_time": "2019-05-28T22:30:42.688161Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = con.query('''\n",
    "    USE FactMap;\n",
    "\n",
    "    SET `compiler.joinmemory` \"128MB\";\n",
    "\n",
    "    DROP DATASET urljoin IF EXISTS;\n",
    "    DROP TYPE PostReviewType IF EXISTS;\n",
    "\n",
    "    CREATE TYPE PostReviewType as {\n",
    "        r: ReviewsType,\n",
    "        p: SubmissionType\n",
    "    };\n",
    "\n",
    "    CREATE DATASET urljoin(PostReviewType)\n",
    "        PRIMARY KEY r.uid, p.id;\n",
    "\n",
    "    INSERT INTO urljoin\n",
    "    SELECT *\n",
    "    FROM posts p, reviews r\n",
    "    WHERE r.claimAuthor.claimURL = p.url;\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T22:38:25.534337Z",
     "start_time": "2019-05-28T22:38:22.943641Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches: 14325\n"
     ]
    }
   ],
   "source": [
    "response = con.query('''\n",
    "    USE FactMap;\n",
    "    \n",
    "    SELECT u.*\n",
    "    FROM urljoin u;\n",
    "    ''')\n",
    "\n",
    "print('Number of matches:', len(response.results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how many unique claimreviews are represented?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T22:38:29.482176Z",
     "start_time": "2019-05-28T22:38:29.028870Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique claims: 1652\n"
     ]
    }
   ],
   "source": [
    "response = con.query('''\n",
    "    USE FactMap;\n",
    "\n",
    "    SELECT count(distinct r.uid) as c\n",
    "    FROM urljoin u\n",
    "    LIMIT 1;\n",
    "    ''')\n",
    "print('Number of unique claims:', response.results[0]['c'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1652 unique claims are posted across the 14325 reddit submissions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And how many of the ratings are numerically valid/invalid?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T22:38:32.271569Z",
     "start_time": "2019-05-28T22:38:32.106199Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique, valid numerical ratings: 923\n"
     ]
    }
   ],
   "source": [
    "response = con.query('''\n",
    "    USE FactMap;\n",
    "\n",
    "    SELECT count(distinct r.uid) as c\n",
    "    FROM urljoin u\n",
    "    WHERE \n",
    "    (r.reviewRating.worstRating < r.reviewRating.bestRating \n",
    "    AND\n",
    "    r.reviewRating.worstRating <= r.reviewRating.ratingValue\n",
    "    AND\n",
    "    r.reviewRating.ratingValue <= r.reviewRating.bestRating)\n",
    "    LIMIT 1;\n",
    "    ''')\n",
    "print('Number of unique, valid numerical ratings:', response.results[0]['c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T22:38:34.823345Z",
     "start_time": "2019-05-28T22:38:34.713492Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique, invalid numerical ratings: 723\n"
     ]
    }
   ],
   "source": [
    "response = con.query('''\n",
    "    USE FactMap;\n",
    "\n",
    "    SELECT count(distinct r.uid) as c\n",
    "    FROM urljoin u\n",
    "    WHERE \n",
    "    NOT\n",
    "    (r.reviewRating.worstRating < r.reviewRating.bestRating \n",
    "    AND\n",
    "    r.reviewRating.worstRating <= r.reviewRating.ratingValue\n",
    "    AND\n",
    "    r.reviewRating.ratingValue <= r.reviewRating.bestRating)\n",
    "    LIMIT 1;\n",
    "    ''')\n",
    "print('Number of unique, invalid numerical ratings:', response.results[0]['c'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard join (reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T22:42:20.695756Z",
     "start_time": "2019-05-28T22:39:49.375893Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = con.query('''\n",
    "    USE FactMap;\n",
    "\n",
    "    SET `compiler.joinmemory` \"128MB\";\n",
    "\n",
    "    DROP DATASET facturljoin IF EXISTS;\n",
    "    \n",
    "    CREATE DATASET facturljoin(PostReviewType)\n",
    "        PRIMARY KEY r.uid, p.id;\n",
    "\n",
    "    INSERT INTO facturljoin\n",
    "    SELECT *\n",
    "    FROM posts p, reviews r\n",
    "    WHERE r.reviewUrl = p.url;\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T22:44:27.005808Z",
     "start_time": "2019-05-28T22:44:20.497093Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches: 19224\n"
     ]
    }
   ],
   "source": [
    "response = con.query('''\n",
    "    USE FactMap;\n",
    "    \n",
    "    SELECT u.*\n",
    "    FROM facturljoin u;\n",
    "    ''')\n",
    "\n",
    "print('Number of matches:', len(response.results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T22:54:06.840610Z",
     "start_time": "2019-05-28T22:54:06.688040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique claims 8636\n"
     ]
    }
   ],
   "source": [
    "response = con.query('''\n",
    "    USE FactMap;\n",
    "\n",
    "    SELECT COUNT(DISTINCT r.uid) unique_claims\n",
    "    FROM facturljoin f;\n",
    "    ''')\n",
    "\n",
    "print('Number of unique claims', response.results[0]['unique_claims'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there's actually more corrected news posted to reddit than fake news!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T23:04:49.994948Z",
     "start_time": "2019-05-28T23:04:49.726972Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response = con.query('''\n",
    "    USE FactMap;\n",
    "\n",
    "    SELECT COUNT(r.uid) as c, g\n",
    "    FROM facturljoin f\n",
    "    GROUP BY r.uid\n",
    "    GROUP AS g\n",
    "    ORDER BY c desc\n",
    "    LIMIT 5;\n",
    "    ''').results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T23:04:50.263397Z",
     "start_time": "2019-05-28T23:04:50.253624Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cross-posts: 36\n",
      "Subreddits:\n",
      " ['DieOff', 'worldpolitics', 'u_the_foreign_code', 'TrumptASStic', 'CringeAnarchy', 'gogopgo', 'u_the_economy_club', 'politics', 'TrumpTellsTales', 'redacted', 'LyingTrump', 'Laserlike_Hodgepodge', 'topofreddit', 'worldnews', 'democrats', 'POLITIC', 'u_talesofafallenforest', 'POLITIC', 'POLITIC', 'POLITIC', 'chomsky', 'LeftCentral', 'politics', 'uspolitics', 'worldpolitics', 'DieOff', 'redacted', 'moderatepolitics', 'Libertarian', 'Impeach_Trump', 'Fuck45', 'esist', 'POLITIC', 'EnoughTrumpSpam', 'NoShitSherlock', 'POLITIC']\n",
      "\n",
      "Number of cross-posts: 36\n",
      "Subreddits:\n",
      " ['DieOff', 'CringeAnarchy', 'worldpolitics', 'u_the_economy_club', 'TrumptASStic', 'TrumpTellsTales', 'gogopgo', 'u_the_foreign_code', 'politics', 'redacted', 'politics', 'worldpolitics', 'DieOff', 'redacted', 'Libertarian', 'uspolitics', 'LyingTrump', 'moderatepolitics', 'topofreddit', 'worldnews', 'esist', 'democrats', 'POLITIC', 'u_talesofafallenforest', 'POLITIC', 'LeftCentral', 'Impeach_Trump', 'Laserlike_Hodgepodge', 'Fuck45', 'POLITIC', 'EnoughTrumpSpam', 'POLITIC', 'NoShitSherlock', 'POLITIC', 'POLITIC', 'chomsky']\n",
      "\n",
      "Number of cross-posts: 29\n",
      "Subreddits:\n",
      " ['The_Donald', 'EcoInternet', 'Conservative', 'nottheonion', 'politics', 'greatawakening', 'The_Donald', 'politics', 'LiarLiar', 'politics', 'The_Donald', 'The_Donald', 'The_Donald', 'worldnews', 'HRCDiscussion', 'The_Donald', 'politics', 'The_Donald', 'The_Donald', 'AnythingGoesPolitics', 'politics', 'The_Donald', 'politics', 'conspiracy', 'The_Donald', 'newjersey', 'politics', 'politics', 'POLITIC']\n",
      "\n",
      "Number of cross-posts: 26\n",
      "Subreddits:\n",
      " ['hillaryclinton', 'conspiracy', 'NorthAmerican', 'politics', 'BernieSandersVetting', 'The_Donald', 'SandersForPresident', 'WhiteRights', 'politics', 'sjwhate', 'politics', 'The_Donald', 'democrats', 'politics', 'Conservative', 'conservatives', 'thedonald', 'BernieSandersVetting', 'socialism', 'enoughsandersspam', 'progressive', 'SocialJusticeInAction', 'sjsucks', 'ThisIsNotASafeSpace', 'SandersForPresident', 'Enough_Sanders_Spam']\n",
      "\n",
      "Number of cross-posts: 26\n",
      "Subreddits:\n",
      " ['SandersForPresident', 'WhiteRights', 'NorthAmerican', 'politics', 'socialism', 'politics', 'enoughsandersspam', 'Conservative', 'progressive', 'conservatives', 'hillaryclinton', 'conspiracy', 'politics', 'sjwhate', 'BernieSandersVetting', 'The_Donald', 'The_Donald', 'politics', 'democrats', 'SocialJusticeInAction', 'SandersForPresident', 'thedonald', 'BernieSandersVetting', 'sjsucks', 'ThisIsNotASafeSpace', 'Enough_Sanders_Spam']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in response:\n",
    "    print(\"Number of cross-posts:\", i['c'])\n",
    "    \n",
    "    subs = [p['f']['p']['subreddit'] for p in i['g']]\n",
    "    print(\"Subreddits:\\n\", subs)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On further inspection, there's no need for further fuzzy matching on the review matches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard join w/ fuzzy join on Twitter/Wikipedia (claims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More refined join, using hard URL join only except for wikipedia and twitter posts, since these URLs to these sites are not claim specific and may refer to multiple tweets/sections.\n",
    "\n",
    "Additional requirements for Twitter/Wikipedia posts include:\n",
    "- The post domain must contain either `wikipedia` or `twitter`.\n",
    "- Minimum caption length is 15 characters (cannot meaningfully distinguish claims below this threshold).\n",
    "- Similar-length claim and post titles must have at least 20% word tokens in common.\n",
    "    - Similar-length is defined as the difference in length between the two titles does not exceed 20% of the shortest title.\n",
    "- Different-length claim and post titles must be contained within one or the other, changing at most 50% of the characters to generate a perfect match to the containing title.\n",
    "    - Different-length is defined as the difference in length between the two titles must be at least 20% of the shortest title."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create table for fuzzy URL join:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T22:45:30.326996Z",
     "start_time": "2019-05-28T22:45:30.148620Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = con.query('''\n",
    "    USE FactMap;\n",
    "\n",
    "    SET `compiler.joinmemory` \"128MB\";\n",
    "\n",
    "    DROP DATASET fuzzyurljoin IF EXISTS;\n",
    "    \n",
    "    CREATE DATASET fuzzyurljoin(PostReviewType)\n",
    "        PRIMARY KEY r.uid, p.id;\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get URL matches with same-length review and post titles (Jaccard similarity > 20%):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T22:45:34.347989Z",
     "start_time": "2019-05-28T22:45:33.810172Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response = con.query('''\n",
    "        USE FactMap;\n",
    "        \n",
    "        INSERT INTO fuzzyurljoin\n",
    "        SELECT u.*\n",
    "        FROM urljoin u\n",
    "        WHERE\n",
    "            (similarity_jaccard(word_tokens(lower(p.title)), word_tokens(lower(r.claimReviewed))) > 0.20\n",
    "            OR similarity_jaccard(word_tokens(lower(p.title)), word_tokens(lower(r.claimReviewed_en))) > 0.20)\n",
    "            AND (abs(length(r.claimReviewed) - length(p.title)) <= \n",
    "                (array_min([length(r.claimReviewed), length(p.title)])) * 0.2)\n",
    "            AND (array_min([length(r.claimReviewed), length(p.title)]) > 15)\n",
    "            AND (contains(p.domain, \"wikipedia\") OR contains(p.domain, \"twitter\"));\n",
    "        ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "= 227 matches added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get URL matches with different-length review and post titles (edit distance > 0.5 * min(review or post title length)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T22:46:02.775591Z",
     "start_time": "2019-05-28T22:46:01.847071Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response = con.query('''\n",
    "    USE FactMap;\n",
    "    \n",
    "    INSERT INTO fuzzyurljoin\n",
    "    SELECT u.*\n",
    "    FROM urljoin u\n",
    "    WHERE\n",
    "            (\n",
    "                (edit_distance_contains(lower(p.title), lower(r.claimReviewed), length(r.claimReviewed) * 0.5)[0] \n",
    "                    OR edit_distance_contains(lower(r.claimReviewed), lower(p.title), length(p.title) * 0.5)[0])\n",
    "                OR\n",
    "                (edit_distance_contains(lower(p.title), lower(r.claimReviewed_en), length(r.claimReviewed_en) * 0.5)[0] \n",
    "                    OR edit_distance_contains(lower(r.claimReviewed_en), lower(p.title), length(p.title) * 0.5)[0])\n",
    "            )\n",
    "            AND (abs(length(r.claimReviewed) - length(p.title)) > \n",
    "                (array_min([length(r.claimReviewed), length(p.title)])) * 0.2)\n",
    "            AND (array_min([length(r.claimReviewed), length(p.title)]) > 15)\n",
    "            AND (contains(p.domain, \"wikipedia\") OR contains(p.domain, \"twitter\"));\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "= 1166 matches added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T22:46:24.759138Z",
     "start_time": "2019-05-28T22:46:24.598118Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response = con.query('''\n",
    "    USE FactMap;\n",
    "    \n",
    "    INSERT INTO fuzzyurljoin\n",
    "    SELECT u.*\n",
    "    FROM urljoin u\n",
    "    WHERE NOT (contains(p.domain, \"wikipedia\") OR contains(p.domain, \"twitter\"));\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "= 6809 matches added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T22:46:43.465943Z",
     "start_time": "2019-05-28T22:46:43.404965Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches: 8202\n"
     ]
    }
   ],
   "source": [
    "response = con.query('''\n",
    "    USE FactMap;\n",
    "    \n",
    "    SELECT COUNT(*) as total\n",
    "    FROM fuzzyurljoin u;\n",
    "    ''')\n",
    "\n",
    "print('Number of matches:', response.results[0]['total'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From 14325 matches to 8202. A reduction of 6323 false matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T22:46:57.441625Z",
     "start_time": "2019-05-28T22:46:57.324951Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique claims 1378\n"
     ]
    }
   ],
   "source": [
    "response = con.query('''\n",
    "    USE FactMap;\n",
    "\n",
    "    SELECT COUNT(DISTINCT r.uid) unique_claims\n",
    "    FROM fuzzyurljoin f;\n",
    "    ''')\n",
    "\n",
    "print('Number of unique claims', response.results[0]['unique_claims'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From 1652 unique claims to 1378. A reduction of 274 falsely matched claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T22:47:11.948445Z",
     "start_time": "2019-05-28T22:47:11.819079Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique, valid numerical ratings: 725\n"
     ]
    }
   ],
   "source": [
    "response = con.query('''\n",
    "    USE FactMap;\n",
    "\n",
    "    SELECT count(distinct r.uid) as c\n",
    "    FROM fuzzyurljoin u\n",
    "    WHERE \n",
    "    (r.reviewRating.worstRating < r.reviewRating.bestRating \n",
    "    AND\n",
    "    r.reviewRating.worstRating <= r.reviewRating.ratingValue\n",
    "    AND\n",
    "    r.reviewRating.ratingValue <= r.reviewRating.bestRating)\n",
    "    LIMIT 1;\n",
    "    ''')\n",
    "print('Number of unique, valid numerical ratings:', response.results[0]['c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T22:47:13.805730Z",
     "start_time": "2019-05-28T22:47:13.696935Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique, invalid numerical ratings: 647\n"
     ]
    }
   ],
   "source": [
    "response = con.query('''\n",
    "    USE FactMap;\n",
    "\n",
    "    SELECT count(distinct r.uid) as c\n",
    "    FROM fuzzyurljoin u\n",
    "    WHERE \n",
    "    NOT\n",
    "    (r.reviewRating.worstRating < r.reviewRating.bestRating \n",
    "    AND\n",
    "    r.reviewRating.worstRating <= r.reviewRating.ratingValue\n",
    "    AND\n",
    "    r.reviewRating.ratingValue <= r.reviewRating.bestRating)\n",
    "    LIMIT 1;\n",
    "    ''')\n",
    "print('Number of unique, invalid numerical ratings:', response.results[0]['c'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpful queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T03:35:08.167090Z",
     "start_time": "2019-04-23T03:35:08.045Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# r = con.query('''\n",
    "#     use FactMap;\n",
    "#     SELECT VALUE ds FROM Metadata.`Dataset` ds WHERE DataverseName = 'FactMap';\n",
    "#     SELECT VALUE ds FROM Metadata.`Dataset` ds;\n",
    "#     SELECT VALUE ix FROM Metadata.`Index` ix;''')\n",
    "\n",
    "# r.results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
